{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Chorse Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Probability and statisitcs\n",
    "4. Hypothesis Testing &nbsp; [1](http://localhost:8888/notebooks/Modeling/AB_testing_morning_students.ipynb) &nbsp;[2](http://localhost:8888/notebooks/Modeling/AB_testing_afternoon.ipynb)\n",
    "2. SQL\n",
    "3. Sampling and Estimating\n",
    "5. Power Calculation and Bayes\n",
    "6. Linear Regression\n",
    "7. Cross Validation and Regularization\n",
    "8. Logistic Regression\n",
    "9. Decision Trees and KNN\n",
    "10. Bagging and Random Forest\n",
    "11. SVM and Kernels [1](http://localhost:8888/notebooks/Modeling/support-vector-machines-fonnesbeck.ipynb)\n",
    "12. Boosting&nbsp;[1](http://localhost:8888/notebooks/Modeling/boosting-drury.ipynb)\n",
    "16. Kmeans and Hierarchical Clustering [1](http://localhost:8888/notebooks/Modeling/Hierarchical_Clustering.ipynb)\n",
    "15. Neural Nets\n",
    "17. Dimensionality Reduction, PCA, and SVD [pca](http://localhost:8888/notebooks/Modeling/PCA%20and%20Higgs%20Boson%20Kaggle%20comp.ipynb)  [SVD](http://localhost:8888/notebooks/Modeling/svd-example-blank.ipynb)\n",
    "19. Non-negative Matrix Factorization (NMF) [1](http://localhost:8888/notebooks/Modeling/nmf_pres.ipynb)\n",
    "20. Natural Language Processing\n",
    "13. Web Scraping\n",
    "14. Mongo DB\n",
    "18. MapReduce and Profit Curve [MapReduce](http://localhost:8888/notebooks/MapReduce/generators_and_mrjob.ipynb)\n",
    "20. AWS\n",
    "21. Spark\n",
    "22. Spark ML\n",
    "23. Recommendation System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Probability and Statistics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Probability </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Value: &nbsp;&nbsp;&nbsp;&nbsp;$\\sum(X) = x_1*p_1 + x_2*p_2 ..... +x_n*p_n$\n",
    "\n",
    "\n",
    "Variance =&nbsp;&nbsp;&nbsp;&nbsp; $\\sum_{i=1}^k = p_i * (x_i - \\mu)^2$\n",
    "\n",
    "\n",
    "\n",
    "COV(X,Y) = &nbsp;&nbsp;&nbsp;&nbsp;$E(X Y)-E(X)E(Y)$\n",
    "\n",
    "\n",
    "\n",
    "P(A $\\bigcap$ B)&nbsp;&nbsp;&nbsp;&nbsp; = $P(A|B) * P(B)\n",
    "                = P(B|A) * P(A)$\n",
    "         \n",
    "         \n",
    "                \n",
    "Bayes Theorm:&nbsp;&nbsp;&nbsp;&nbsp; $P(B|A) =  \\frac{P(A|B) * P(B)}{P(A)}$\n",
    "\n",
    "\n",
    "\n",
    "Permutations =&nbsp;&nbsp;&nbsp;&nbsp;$P(n,k)=\\dfrac{n!}{(n-k)!}$\n",
    "\n",
    "\n",
    "\n",
    "Combinations =&nbsp;&nbsp;&nbsp;&nbsp; $C(n,k)={{n}\\choose{k}} = {{n!}\\over{(n-k)!k!}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Probability Distributions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Discrete </h3>\n",
    "\n",
    "X ~ $Bernoulli$: single coin flip turns out to be heads\n",
    "\n",
    "X ~ $Binomial(n,p)$: number of coin flips out of n that thurn out to be heads\n",
    "\n",
    "X ~ $Geometric(p)$: number traials until a coin flip turns out to be heads\n",
    "\n",
    "X ~ $Poisson(\\lambda=5)$: number of customers arriving in a coffee shop in a give time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Continuous</h3>\n",
    "\n",
    "X ~ $Exponential(\\lambda=5)$: time until customer will arrive at the coffee shop\n",
    "\n",
    "X ~ $Uniform(a=0, b=360)$: locatoin of the hour or minute hand (in degrees) on a clock\n",
    "\n",
    "X ~ $Gaussian(\\mu=100, \\sigma=15)$: IQ Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Discrete Formulas </h3>\n",
    "\n",
    "$Binomial:  P(X=k) = \\binom{n}{k}p^k(1-p)^{k-1}p$\n",
    "\n",
    "$Poisson: P(X=k) = \\frac{\\lambda^ke^{-\\lambda}}{k!}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Continuous Formulas</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Eponential: f(x)=\\lambda*e^{-\\lambda}, x>=0$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $E(X) = \\frac{1}{\\lambda}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Statistics and Hypothesis testing </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Central Limit Theorm:</b><p> The CLT says that the mean of a sufficiently $(>30)$ large number of i.i.d random variables will be approximately normal!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hypothesis Testing</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. State the null hypothesis $(H_0)$ and the alternative hypothesis $(H_A)$\n",
    "2. Choose the significance level, $\\alpha$\n",
    "3. Compute the appropriate test statistic\n",
    "4. Compute the p-values under the assumption that $H_0$ is true\n",
    "    1. if p-value $<= \\alpha \\to$ Reject $H_0$ in favor of $H_A$\n",
    "    2. if p-value $>= \\alpha \\to$ Reject $H_0$ in favor of $H_A$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>significance level ($\\alpha$)</b>: Is the proabability of rejecting the null hypothesis\n",
    "\n",
    "\n",
    "- <b>p-value</b>: The p-value is defined as the probability, under the null hypothesis, of obtaining a result equal to or more extreme than what was actually observed. \n",
    "\n",
    "\n",
    "- <b>Hypothesis errors Errors</b>:\n",
    "    1. Type 1 error: Rejecting $H_0$ when it is true\n",
    "    2. Type 2 error: Falling to reject $H_0$ when it is false\n",
    "    \n",
    "    \n",
    "- <b>Test Statistic</b>: $z = \\frac{\\bar{x}-\\mu_0}{\\sigma_0/\\sqrt{n}}$\n",
    "\n",
    "\n",
    "- <b>Bonferroni Correction</b>: When making mulitple comparisons, need to adjust sign rates of indiviual tests.\n",
    "    - $\\alpha_i = \\frac{\\alpha_E}{m}$\n",
    "    - $m$ is the number of comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example code from scipy stats for a ttest</b>\n",
    "\n",
    "```scipy.stats.ttest_ind(a, b, equal_var = False)```\n",
    "\n",
    "- What is the $P(90<X<95)$ where μμ = 80, σσ = 12?\n",
    "\n",
    "        import scipy.stats as stats\n",
    "        disn = stats.norm(loc=80,scale=12)\n",
    "        disn.cdf(95) - disn.cdf(90)\n",
    "        0.096678607296787789\n",
    "        \n",
    "- <b> Binomial Test </b>\n",
    "\n",
    "    ```print(\"binomial test - %s\"%st.binom_test(h, n, p))```\n",
    "\n",
    "<b>Link to more example code, [Click here](http://file:///Users/jacobshomer/Desktop/Galvanize/power-bayesian/html/review.html)</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Power </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\alpha$ = Type I Error\n",
    "- $\\beta$ = Type II Error\n",
    "- $1-\\beta$ = Power - Reject $H_0$ when $H_0$ is False\n",
    "\n",
    "\n",
    "* Power determines the number of smples needed to power a given study\n",
    "* The likelihood that we call something significant when there is something there ot be deteted \n",
    "\n",
    "<b>Things that affect our statistical power</b>\n",
    "1. Significance level\n",
    "2. $|\\mu_0- \\mu_1|$\n",
    "3. Standard deviation \n",
    "4. Sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Power](https://i.pinimg.com/originals/47/25/2f/47252f82c782ff42864902deb188e7bc.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Linear Regression</h2>\n",
    "\n",
    "[link_to_notebook](http://localhost:8888/notebooks/Modeling/EDA%2Blinear_regression_intro.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Logistic Regression </h2>\n",
    "\n",
    "[link_to_notebook](http://localhost:8888/notebooks/Modeling/Linear_logistic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Cross Validation and Regularization </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is regression??</b>\n",
    "\n",
    "- Use features to predict real valued targets\n",
    "    -future sales/revenue\n",
    "\n",
    "<b>What is Classification</b>\n",
    "\n",
    "- use features to predict cateorical targets\n",
    "    - yes/no, 0-9\n",
    "    \n",
    "<b>Underfitting </b>\n",
    "- The model doesnt't fully capture the relationship between predictor and the target\n",
    "- Has not learned the data's signal\n",
    "\n",
    "     -<b>signal</b> is that what we want to model\n",
    "\n",
    "<b>Overfitting:</b>\n",
    "- The model has tried to capture the sampling error\n",
    "- Has learned the data's signal and the noise\n",
    "\n",
    "     -<b>noise</b> is that what we don't want to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K-Fold</h3>\n",
    "\n",
    "1.Split data set into k 'folds'\n",
    "2. Train using (k-1) folds. Validate on the left out fold. Record validation.\n",
    "3. Train k models, leaving a different fold out each time\n",
    "4. Average the validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K_fold](k-fold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bias/Variance Trade off</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <b>Bias</b>: the amount the expected value of the results differs from the true value\n",
    "    - $Bias[\\hat{f}(x)] = E[\\hat{f}(x) - f(x)]$\n",
    "    \n",
    "2. <b>Variance</b>: the expected value of the squared deviation of the results from the mean of the results. In other words, the spread of the square difference from the mean\n",
    "\n",
    "    - $Var[\\hat{f}(x)] = E[\\hat{f}(x)^2] - E[f(x)]^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A low bias model accurately predicts the populations's underlying true value, or signal\n",
    "\n",
    "\n",
    "- A low variance model's predictions dont't change much when it is fit on different data from the underlying population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias_variance](bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cross Validation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique for assess how the results of statistical analysis will generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](onecross_val.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Modeling <h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>General Modeling Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Curse of Dimensionality</b>: as the number of parameters increases the model the data points need increases exponentially\n",
    "\n",
    "   --Sampling density is proportional to $N^{\\frac{1}{D}}$\n",
    "   \n",
    "   --All distance based metrics fall apart (Linear, Logistic, SVM, KMeans, KNN,)\n",
    "   \n",
    "<b>Regularization</b>: is the processes of restricing our model to reduce its variance and complexity (number of features)\n",
    "\n",
    "<b>Non-Parametric</b>: \n",
    "\n",
    "        -The structure is not specified beforehand, it is determinded from the data\n",
    "        -Models population cannot be described in advanced\n",
    "        -Applicability is wider\n",
    "        -In cases where parametric models are appropriate, non_parametrics have less power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parametric     | Non-Parametric           | \n",
    "| ------------- |:-------------:|\n",
    "|Simple/easy to understand interpret | Flexible |\n",
    "| fast to predict  | No assumption about the functional form  |\n",
    "| Don't need as mcuh data | Good performance    | \n",
    "|Constrained, bias| More Data!|\n",
    "|Poor perfromance if model assumptions do not hold|  Easily overfit |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regularization</h3>\n",
    "\n",
    "<b>Linear Regression</b>: $Y = \\beta_0 +\\beta_1X_1+...+\\beta_pX_p + \\epsilon $\n",
    "\n",
    "Estimation of parameter by minimizing:  $\\sum_{i=1}^{N}(y_i-\\hat{\\beta_0}-\\sum_{j=1}^{p}X_{ij}\\hat{\\beta_j})^2$\n",
    "\n",
    "<b>Regularized Linear Regression</b>: $Y = \\beta_0 +\\beta_1X_1+...+\\beta_pX_p + \\epsilon $\n",
    "\n",
    "Estimation of parameter by minimizing:  $\\sum_{i=1}^{N}(y_i-\\hat{\\beta_0}-\\sum_{j=1}^{p}X_{ij}\\hat{\\beta_j})^2 +\\lambda\\sum_{j=1}^{p}|\\hat{\\beta_j}|^L$\n",
    "\n",
    "Why? Changing $\\lambda$ changes the amount that large coefficients are penalized. Increasing $\\lambda$ increases the model's bas and decreases its variance\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Supervised vs Unsupervised learning </h3>\n",
    "\n",
    "\n",
    "| Supervised     | Unsupervised           | \n",
    "| ------------- |:-------------:|\n",
    "|Have a target/label that we model | No target/label to predict |\n",
    "| takes in features and predicts a label | Goal is to find underlying structure/patterns/organ |\n",
    "|Error metric to compare models | No real error metric    | \n",
    "|Used to predict future unlabeled data|\n",
    "\n",
    "\n",
    "![supervised_vs_unsuper](supervised_vs_unsupervised.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Descision Trees</h2>\n",
    "![descision_tree](descision_tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Decision trees are a supervised, non_parametrics learners that preforms a series of binary splits with the goal of minimizing predictive error. The key point is that each split is determined by the information gain of the split.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Pros        | Cons           | \n",
    "| ------------- |:-------------:|\n",
    "|non-parametric, non-linear      | expensive to train |\n",
    "| classification or regression     | greedy algorithm (local maxima)  |\n",
    "| easy to interpret | Overfits easy    | \n",
    "|computationally cheap predictions| deterministic|\n",
    "|multicollinearity|   |\n",
    "\n",
    "\n",
    "<i><b>Deterministic</b> is that you will get teh same model on the same training data, whether it is good or bad</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How to split</h3>\n",
    "\n",
    "1. Calculate the information gain for all possible splits\n",
    "2. Commit the split that has the highest information gain\n",
    "\n",
    "Shannon Entropy: $H(X) = -\\sum p_ilog_2(p_i)$ \n",
    "\n",
    "        p_i: probability of each possible discrete outcome\n",
    "\n",
    "<b>Information Gain</b> $IG(S,C) = H(S) - \\sum_{C_i\\in C}\\frac{|C_i|}{|S|}H(C_i) $\n",
    "\n",
    "$S$ = The parent's set of samples\n",
    "\n",
    "$C$ = the set of children\n",
    "\n",
    "$C_i$ = the set of samples in each child\n",
    "\n",
    "$H$ = entropy of child i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preventing Overfitting (Pruning)</h3>\n",
    "\n",
    "-Leaf size: stop sploitting when number of samples gets small enough\n",
    "\n",
    "-Depth: stop splitting at a certain depth\n",
    "\n",
    "-purity: stop splitting if enough of the examples are the sme class\n",
    "\n",
    "-gain threshold: stop splitting when the info gain is too small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>K Nearest Neighbors (KNN)</h2>\n",
    "\n",
    "![knn](knn.png)\n",
    "\n",
    "\n",
    "[Link_To_notebook](http://localhost:8888/notebooks/Modeling/knn_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>KNN Classification</b>: The output is class membership. Each sample is assigned ot the class nearest to it\n",
    "\n",
    "<b>KNN regression</b>: The output is the predicted value for the sample. This value is the average of teh values of <i>k</i> nearest training samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "\n",
    "    1. Calculate the distance from x to all points in your dataset\n",
    "    2. Sort the points in your dataset by increasing distance from x\n",
    "    3. Predict the majority (mode) label of the k nearest points (or average of k if regression)\n",
    "    \n",
    "<b>Distance Metrics</b>\n",
    "\n",
    "Euclidean Dist (L2): $\\sqrt{\\sum_{i=1}^n(q_i-p_i)^2}$\n",
    "\n",
    "Manhattan Distance(L1): $\\sum_{i=1}^n|q_i-p_i|$\n",
    "\n",
    "Cosince Distance = 1 - Cosine Similarity = $1- \\frac{a*b}{||a||*||b||}$\n",
    "\n",
    "<b>Hyperparameter</b> The only hyperparameter is k. As as rule of thumb start with $k = \\sqrt{n}$\n",
    "\n",
    "\n",
    "\n",
    "<b>Curse of Dimensionality</b>: KNN will suffer in high dimensions. It takes much data to make up for increased dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Pros        | Cons           | \n",
    "| ------------- |:-------------:|\n",
    "|super-simple     | high prediction cost |\n",
    "| works with any number of classes     | High dimns= poor performance  |\n",
    "| easy to add data | Categorical features don't work well   | \n",
    "|few hyperparameters|\n",
    "|multicollinearity|  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Ensemble, Bagging and Random Forest </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Ensemble</h3>\n",
    "\n",
    "-Combine multiple weak learners to make a strong learner\n",
    " \n",
    "    1. Train muliple models on the data\n",
    "    2. predict and take the average of the different models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bootstrapping and Bagging</h3>\n",
    "\n",
    "-Given a sample of n data points, we take B bootstrapped samples of our data with replacement\n",
    "-We can use it to get a confidence intervals around a statistic/parameter\n",
    "\n",
    "<b>Bagging (boostrap aggregation)</b>: We need independent sample to train our models on!\n",
    "\n",
    "    1.Draw new samples from our population ( or bootstrap)\n",
    "    2. Train a model on each new sample\n",
    "    3. Average the predictions from each (aggregating)\n",
    "    \n",
    "\n",
    "-General purpose methon for reducing the variance\n",
    "\n",
    "-Trees are most comming. Grow them deep -- high variance, low bias\n",
    "\n",
    "-Aggregating the ouputs gives us large reduction in variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest</h3>\n",
    "\n",
    "-improves over bagging becase each tree is decorrelated. \n",
    "-Each split only m featurns are selected $m = sqrt(p)$\n",
    "\n",
    "<b>hyperparameters</b>:\n",
    "\n",
    "-Number of trees in the forest = n_estimators\n",
    "\n",
    "-Information gain metric = criterion\n",
    "\n",
    "-Number of features to consider for split = max_features\n",
    "\n",
    "\n",
    "<b>Out of bag Score</b>: a quick replacment for cross validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Support Vecotor Machine (SVM) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Boosting </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> General idea </h3>\n",
    "\n",
    "- <b> An ensemble of trees are groun sequentially based on the information learned from prevous tree</b>\n",
    "\n",
    "    1. Each tree is fit on some modified verions of the training set based on the previous resluts\n",
    "    2. In <b>adaboost</b> the datapoints associated with the largest residuals are weighted the most in the new training set.\n",
    "    \n",
    "    3. in <b>gradient Boosting</b> the new training set is the residulas\n",
    "\n",
    "\n",
    "\n",
    "- The different between averaging methonds (random forest, bagging) and boosting\n",
    "    - With averaging models you take multiple weak learners and average the predictions\n",
    "    \n",
    "    - With boosting, high bias models are built seqentially on top of each other\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gradient Boost Algorithm</h3>\n",
    "\n",
    "![gradient_boost](gradient_boost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hyperparameters</b>\n",
    "\n",
    "1. Learning rate (shrinkage parameter): typically around 0.2. Small LR lead to generalization of model\n",
    "2. n_estimators: The number of sequential models in boosting\n",
    "3. max_depth: Depth of each tree. This can help with over fitting (2-8)\n",
    "\n",
    "![grid_search](grid_search_rf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Adaboost</h3>\n",
    "\n",
    "![adaboost](adaboost.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>KMeans</h2>\n",
    "\n",
    "![kmeans](kmeans.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Basic Algorithm</b>\n",
    "\n",
    "```Initialize k centroids\n",
    "    while not converged:\n",
    "    assign to new centroid \n",
    "    compute new centriod means\n",
    "```\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "Three differnt ways to initialize centriods:\n",
    "   \n",
    "   1. Randomly choose k points from your data\n",
    "   2. Randomly assign each data point to a number 1-k and initialize the kth centriod to the average of the points with kth label\n",
    "   3. k-means++ -irst centroid is chosen at random, with subsequent centroids chosen with probability proportional to the squared distance to the closest existing centroid\n",
    "\n",
    "\n",
    "Stopping Criteria (while not converged):\n",
    "\n",
    "    1. specified number of iterations\n",
    "    2. Until the centriods are unchanged\n",
    "    3. Defined distance of centroids\n",
    "\n",
    "\n",
    "Evaluating K-means:\n",
    "\n",
    "Within Cluster Variance: $WCV(C_k) = \\frac{1}{|C_k|} = \\sum_{i,i` \\in C_k}\\sum_{j=1}^p(X_{ij}-x_{i`j})^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hierarchical Clustering </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Nets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dimensionality Reduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Different ways to reduce dimensionality\n",
    "1. Lasso Regression\n",
    "2.Relaxed Lasso:\n",
    "    -Do lasso, throw away unused features, then OLS regression\n",
    "3. Neural Networks\n",
    "4. Principle Components Analysis (PCA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Principle Components Analysis</h3>\n",
    "\n",
    "<b>Derivation</b>\n",
    "\n",
    "![pca](pca1.png)\n",
    "![pca2](pca2.png)\n",
    "![pca3](pca3.png)\n",
    "![pca4](pca4.png)\n",
    "![pca](pca5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- The relative size of the eigenvalues tell us the % of explained variance of each eigenvector\n",
    "- if $\\lambda_1,...,\\lambda_p$ are the eigenvalues in decreasing magnitude then the variance explained by k eignevectors.\n",
    "\n",
    "    $$\\frac{\\sum_{i=1}^k\\lambda_i}{\\sum_{j=1}^p\\lambda_j}$$\n",
    "    \n",
    "-Typically set k in order to capture 90% of variance\n",
    "\n",
    "\n",
    "<b>In Practice</b>\n",
    "\n",
    "1. Choose first k Eigenvectors:  <h3>$$V_{p x k}$$</h3>\n",
    "\n",
    "2. Reduce dimensionality of data to k dims: <h3>$$R_{nxk} = M_{nxp}*V_{pxk}$$</h3>\n",
    "\n",
    "3. Reconstruct approxitmated M: <h3>$$M=RV^T$$</h3>\n",
    "\n",
    "\n",
    "<b>Summary</b>\n",
    "1. Create the centered design matrix\n",
    "    -Center your data by subtracting the mean. Each row becaomes one sample (M)\n",
    "2. Standardize M (divide by std)\n",
    "3. Compute the covariance matrix $\\sum$\n",
    "4. The principal components are the eigenvectors of the covariance matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Singular Value Decomposition</h3>\n",
    "![svd](svd1.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<b>Overview</b>\n",
    "\n",
    "$U_{nxn}$: Columns are the eigenvectors of $MM^T$\n",
    "\n",
    "$V_{pxp}$ Columns are the eigenvectors of $M^TM$\n",
    "\n",
    "We can reduce the dimensions fo M by choosing d < k \n",
    "\n",
    "\n",
    "![svd](svd2.png)\n",
    "\n",
    "\n",
    "\n",
    "<b>Capturing Latent Features</b>\n",
    "\n",
    "Some intuition: 'Say our dataset maps from space X to space Y. E.g. X might be 'user space' and Y might be 'movie space.' After doing SVD, we can interpret the SVD as mapping from X space to a 'comcept space', then mapping to Y space.\n",
    "\n",
    "\n",
    "<b>Returned By SVD</b>\n",
    "\n",
    "- SVD returns three matrices U,S and V\n",
    "    - U: contains eigenvectors that are linear combinations of the original features, orthogonal to other rows. The size is n_columns, by k_latent_features\n",
    "    - S: contains the singular values, or the square root of the eigenvalues associated with the eigenvectors represented in S. The square is the variance explained by this vector. (KxK)\n",
    "    - V: Contains eigenvectors that are linear combinations of the original rows (K_latent_features, p_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Non-negative Matrix Factorization (NMF)</h2>\n",
    "\n",
    "<b>Overview</b>\n",
    "\n",
    "- Take a large matrix (V) and factor into two smaller dimensional matrices W and H\n",
    "- Force solutions to have all non-negative solutions\n",
    "- Creates a 'parts based representation'\n",
    "- NMF is a method for modeling the gneration fo directly observable bisible variables V from hidden variables H\n",
    "\n",
    "\n",
    "<b>Parts based Representation</b>\n",
    "- The non_negatibity constraints allows for additive combinations. Parts are amplified \n",
    "- Combining parts to make whole\n",
    "- Differs from PCA encoding with negative components which detract from components already in place\n",
    "\n",
    "<b>NMF Algorithm</b>\n",
    "\n",
    "<h3>$$V_{n*m} \\approx W_{N*P}*H_{P*M}$$</h3>\n",
    "\n",
    "* all elements of W and H must be non-negative\n",
    "* $p <= min(m,n)$\n",
    "\n",
    "Alternating Least Squares Method:\n",
    "\n",
    "* Start with a random W\n",
    "* Repeatedly solve for H, W to minimize cost function (least squares)\n",
    "\n",
    "![nmf](nmf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Profit Curve</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Natural Language Processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Bases</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SQL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Postgres SQL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mongo DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Web Scrapping </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>AWS</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spark</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Aspects of Spark</b>\n",
    "- Open-source cluster computing framework\n",
    "- Successor to Hadoop Map Reduce\n",
    "- Pyspark is the API for python\n",
    "\n",
    "\n",
    "<h3>Resilient Distributed Dataset</h3>\n",
    "\n",
    "- Distributed Collection\n",
    "- Fault Tolerant\n",
    "- Parallel operation - Partitioned\n",
    "\n",
    "<b>Properties</b>\n",
    "- Immutable\n",
    "- Lazily Evaluated\n",
    "- Cachable\n",
    "\n",
    "<b>Operations</b>\n",
    "- Actions\n",
    "    - collect - return all elements of the RDD as an array \n",
    "    - Count -  return the number of elements in the RDD\n",
    "    - First - returns the first element\n",
    "    - take - return an array with the first n elements of RDD\n",
    "    \n",
    "- Transformations\n",
    "    - Filter - applies a function to each element and returns elements that are evaluated as true\n",
    "    - Map - transforms each element\n",
    "    - FlatMap - transforms each element into 0-N elements\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MapRreduce and Apache Hadoop</h2>\n",
    "\n",
    "<b>Hadoop</b>\n",
    "\n",
    "- open-source, <b>distributed computing framework</b>\n",
    "- provides a method to access and process data that are distributed among multiple clustered computers\n",
    "- <b>Hadoop Distributed File System</b> manages and provides access to distributed data\n",
    "- <b>Hadoop MapReduce</b> parallel processing system for large datasets\n",
    "\n",
    "<h3>MapReduce</h3>\n",
    "\n",
    "- Sends the computaion to the data rather than other way\n",
    "- Split one large task into many smaller sub-task that can be solved in paralle\n",
    "- Solve these tasks independently\n",
    "- Recombine the results of the sub-tasks for the final result\n",
    "\n",
    "<b>Especially suited for</b>: count, sum, avg, sort, graph traversal, and some machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mapreduce](mapreduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Take Aways</h3>\n",
    "\n",
    "* Hadoop - open source framework made to handle big data through distributed computing\n",
    "* HDFS - data managment component of Hadoop\n",
    "    - NameNode - keeps track of where data is, makes sure its backed up\n",
    "    - DataNode - Stores the data\n",
    "* MapReduce - Computation component of Hadoop\n",
    "    - JobTracker - coordinates jobs, communicates with client\n",
    "    - TaskTracker - performs computations on local data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spark ML</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recommendation System</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b> Similarity Metrics</b>\n",
    "\n",
    "- Euclidean Distance: <h3>$$similarity(a,b) = \\frac{1}{1+dist(a,b)}$$</h3>\n",
    "$$dist(a,b) = ||a-b|| = \\sqrt{\\sum_i(a_i-b_i)^2}$$\n",
    "\n",
    "- Pearson Correlation: <h4>$$pearson(a,b) = \\frac{cov(a,b)}{std(a)*std(b)} = \\frac{\\sum_i(a_i-\\bar{a})(b_i-\\bar{b})}{\\sqrt{\\sum_i(a_i-\\bar{a})^2}\\sqrt{\\sum_i(b_i-\\bar{b})^2}}$$</h4>\n",
    "$$similarity(a,b) = 0.5 + 0.5 * pearson(a,b)$$\n",
    "\n",
    "\n",
    "- Cosine Similarity:$$cos(\\theta_{a,b}) = \\frac{a*b)}{||a||*||b||}= \\frac{\\sum_ia_Ib_i}{\\sqrt{\\sum_ia_i^2}\\sqrt{\\sum_ib_i^2}}$$\n",
    "\n",
    "$$similarity(a,b) = 0.5 + 0.5*cos(\\theta_{a,b})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Making predictions</h3>\n",
    "\n",
    "Say user u hasn't rated item i...\n",
    "\n",
    "![predictions](recommender.png)\n",
    "\n",
    "\n",
    "Order by descending predicted rating for a single user and recommend the top k items to the user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
